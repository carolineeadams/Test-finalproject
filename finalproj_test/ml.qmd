# Machine Learning

## Executive Summary

Our machine learning analysis provided insights into working with volatile stock data. The logistic regression and random forest algorithms yielded an accuracy score of 84%, suggesting that variables like comment score, controversiality, length, and sentiment were helpful predictors of a post's associated stock. However, the data was unbalanced, possibly influencing the high accuracy score. The first two modeling exercises were not promising, with little predictiveness found in submissions or comments. Overall, predicting sentiment in Reddit submissions discussing "basic girl" stocks is difficult due to varying sentiments and similar engagement metrics. These insights can inform future research in this field.

## Analysis Report + Business Questions

9.  **Explore whether or not we can predict the sentiment of a submission based on engagement of the post and the stock associated with it.**

*Model 1: Predicting Submission Sentiment from Information on Post Engagement, Comments, Types of Stocks Mentioned in the Post, and Stock Prices*

Initially, we aimed to "explore whether or not we can predict stock price directionality based on submission and comment data from Reddit". However, after further diving into our data we realized this was unfeasible. The data on Reddit is user-generated content and may contain inaccurate or biased information. Meanwhile, the stock market is influenced by a wide range of factors, including economic indicators, geopolitical events, and company news, making it difficult to isolate the impact of Reddit data on stock price directionality. Due to this, even if there is some correlation it may be difficult to identify and quantify, which makes this a difficult model to build and trust with the data we have.

Instead, we have decided to leverage our data on submissions and the associated stock types to predict the sentiment of each submission as we believe this will be more relevant. By analyzing engagement, we can gain valuable insights into consumer sentiment and preferences, particularly in the "basic girl" demographic. This will help us better understand which types of submissions are likely to generate positive responses and how to communicate about certain stocks in a way that will capture the attention of this audience.

We generated a machine learning pipeline using PySpark to perform binary classification on the set of Reddit submissions that mentioned at least one "basic girl" stock in the post text or the title. The dataset contained features such as 'score,' 'num_comments,' and 'submission_length,' and binary indicator variables for companies such as 'lululemon' and 'apple.' The dummy variables indicated if a given basic stock was mentioned in the submission. If a given basic stock was mentioned in the submission, we also included the share price of that stock at the time that the submission was posted as a predictor. For days when the stock market was closed, the average share price of the stock over the entire time period (January 2022 through January 2023) was imputed. The goal of the pipeline was to predict whether a given piece of text had a positive or negative sentiment, which was generated in our natural language processing module of the project.

The pipeline consisted of several stages, and we used this pipeline process for *both* of our models (see second model below). First, the text sentiment was converted from a string to a numeric index using the StringIndexer function. Next, the features were assembled into a vector using the VectorAssembler function. Two machine learning models were then defined: logistic regression and random forest. The models were then evaluated using k-fold cross-validation and grid search to determine the best set of hyperparameters. This step was included in our comprehensive pipeline to address the extra credit prompt. The logistic regression model was trained with a regularization parameter c of either 0.01 or 0.1, while the random forest model was trained with a different number of trees and maximum depth parameters. Finally, the best models were used to predict sentiment on a test dataset, and their performance was evaluated using the BinaryClassificationEvaluator function, with the area under the receiver operating characteristic curve (AUC) being used as the primary metric. Other metrics were also evaluated for both models to better understand overall performance. Eighty percent of the data was used for training, eighteen percent was used for testing, and two percent was set aside for prediction efforts.

The evaluation results below demonstrated the performance of the logistic regression and random forest models on the test dataset. The confusion matrix shows the number of true positives, true negatives, false positives, and false negatives for each model. The false negative rate for both models was quite high, with 98 instances being classified as negative when they should have been positive for the logistic regression model and 91 for the random forest. Both models had much lower false positive rates, with one instance being classified as positive when it should have been negative for the logistic regression model and 8 for the random forest model.

![](images/ML-Table1.png){fig-align="center" width="375"}

The accuracy for both models was the same at 0.669, indicating that they correctly predicted sentiment in roughly two-thirds of the cases. However, the AUC score for the logistic regression model was only slightly above chance at 0.506, while the AUC score for the random forest model was marginally higher at 0.52. AUC is a metric that measures the overall performance of a binary classifier by plotting the true positive rate against the false positive rate, and a score of 0.5 indicates random guessing. Thus, the AUC scores suggest that both models performed only slightly better than random guessing.

The precision score measures the proportion of predicted positive instances that are actually positive. In contrast, the recall score measures the proportion of true positive instances that are correctly predicted as positive. The precision score for the random forest model was 0.47, meaning that roughly half of the instances classified as positive were actually positive. The recall score for both models was low, indicating that a small proportion of true positive instances were correctly classified as positive. Finally, the F1 score is the harmonic mean of precision and recall, and it was only available for the random forest model, with a value of 0.124. Overall, the results suggest that the models did not perform particularly well at predicting sentiment. 

The ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classifier at different classification thresholds. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. 

ROC curves were plotted for both the random forest and the logistic regression models. These are shown below. We can see that for both, the ROC curve hovered around 0.5, which means that the model did not perform better than random guessing. This is because the area under the ROC curve (AUC) for a model that performs no better than random guessing is 0.5. Therefore, an AUC score close to 0.5 suggests that the model is not performing well in distinguishing between the positive and negative classes.

![](images/ML-M1_All.png){fig-align="center" width="550"}

\[[LINK](https://github.com/gu-ppol567-project/ppol-567-final-project-team-1/blob/main/code/ml/ml1.ipynb)\] - Model 1 Code

*Model 2: Predicting Comment Sentiment from Information on Comment Length, Score, and Controversiality*

We narrowed the comments down to those during which users were most active in posting submissions and comments on the subreddit, which was February 2022. Most comments were generated in the first half of the month, so we filtered the comments only to include those posted in the subreddit between February 1 and February 23, 2022, resulting in a total of 942,612 comments. The dataset contained the features 'score,' 'controversiality,' and 'comment_length.' The goal of the pipeline was to predict whether a given comment had a positive or negative sentiment based on those features, which was generated in our natural language processing module of the project.

The evaluation results below demonstrated the performance of the logistic regression and random forest models on the test dataset. The confusion matrix shows the number of true positives, true negatives, false positives, and false negatives for each model. The false negative rate for both models was again relatively high, with 149,936 instances being classified as negative when they should have been positive for the logistic regression model and 104,335 for the random forest. Both models had much lower false positive rates, with six instances being classified as positive when they should have been negative for the logistic regression model and 34,301 for the random forest model. The logistic regression model had a much lower false positive rate than the random forest model.

![](images/ML-Table2.png){fig-align="center" width="375"}

The accuracy for both models was similar, with the logistic regression model having a slightly worse accuracy than the random forest model (0.523 and 0.559, respectively). The scores indicate they correctly predicted sentiment in over half of the cases. However, the AUC score for the random forest model was the highest of any machine learning model types used in this analysis, despite it being only slightly above chance at 0.589. In contrast, the AUC score for the logistic regression model was a little lower at 0.53. Again, the AUC scores suggest that both models performed only slightly better than random guessing.

The logistic regression model had a high precision score of 0.714, meaning that about seventy percent of the instances classified as positive were actually positive. The recall score for the random forest model was lower at 0.571. The random forest model had a better F1 score (0.397) than the logistic regression model (0), although both models generally had very low F1 scores. Similar to model 1, the results from this analysis suggest that the models did not perform particularly well at predicting comment sentiment.

Lastly, ROC curves were plotted for both the random forest and the logistic regression models. These are shown below. We can see that for both, the ROC curve hovered around or slightly above 0.5, which means that the models were not performing better than random guessing. Therefore, an AUC score of 0.5 suggests that the model is not performing well in distinguishing between the positive and negative classes.

![](images/ML-M2_All.png){fig-align="center" width="550"}

\[[LINK](https://github.com/gu-ppol567-project/ppol-567-final-project-team-1/blob/main/code/ml/ml2.ipynb)\] - Model 2 Code

10. **Predict the industry of the stock being discussed in a submission based on data related to the submission and its comments, such as keywords, sentiment, and context.**

    *Model 3: Predicting the Sector of the Stock Based on Comment Length, Comment Score, Controversiality, and Sentiment*

    Initially, we had questioned "how can we use machine learning algorithms to predict which stocks are likely to be discussed more frequently on stock subreddits in the near future". One of the main issues we found with this question is the limited predictive power. Even with the most advanced machine learning algorithms, it would be difficult to accurately predict which stocks would be discussed more frequently on subreddits in the near future, as social media activity is influenced by a wide range of factors that can be difficult to quantify.

    Also, based on the data we had and the analysis we did in NLP, we felt that there could be a more interesting analysis related to the sentiment data and the impacts of the industry of the stocks. For one, knowing the industry of the stock being discussed in a submission can help businesses target their marketing efforts more effectively, by tailoring their messaging and offerings to specific customer segments and understanding consumer behavior by industry better. Also, understanding the industry of the stock being discussed in a submission can help businesses manage risk more effectively, by identifying potential threats and opportunities in the market.

    We generated a machine learning pipeline using PySpark to perform a binary classification on a data set of Reddit comments that were associated with a submission that mentioned at least one "basic girl" stock in the post text or the title. The data set contained features such as 'score,' 'controversiality,' and 'comment_length,' and binary indicator variables for each company's presence. The dummy variables indicated if a given basic stock was mentioned in the submission. The goal of the model was to predict the industry of the stock discussed in a given comment. We identified the sectors as tech, retail, and food & beverage; and each stock was only given one categorization.

    One of the main issues in this dataset was the imbalanced classes. The positive class (sector was mentioned, in this use case looking at retail) was only 15.37% of the total observations, making the negative class about 85%. To address this, we looking into oversampling the smaller class or undersampling the larger class, such as processes like SMOTE. However at this time we did not feel this aptly applied to our problem, and it negatively affected our models when run. For future work, this could be something to look into more deeply.

    The evaluation results demonstrated that the performance of the logistic regression and random forest models was exactly the same on the test dataset. The false negative rate for both models was high, with 2,665 observations being classified as negative when they should have been positive. Neither model predicted that any observations were positive, and therefore, had true positive and false positive rates of zero.

    The accuracy for both models was the same at 0.847, which was the highest accuracy we achieved from any algorithm applied to our three modeling exercises. The scores indicate the models correctly predicted if a comment was connected to a post that discussed a retail company stock or not in the majority of the cases. However, the AUC scores were still low at 0.512. The AUC scores suggest that both models performed only slightly better than chance at guessing in terms of distinguishing between the two classes. Because the models did not predict any of the positive class, neither one had a precision or F1 score. In addition, both models had recall values of zero.

    ![](images/ML-Table3.png){width="375"}

    ROC curves were plotted for both the logistic regression and the random forest models, as shown below. It is evident that both models performed poorly since the ROC curve is close to 0.5, indicating that the model's performance is no better than random guessing. An area under the ROC curve (AUC) score of 0.5 is typical for a model that performs no better than random guessing. Hence, a score close to 0.5 suggests that the model cannot differentiate well between the positive and negative classes.

![](images/ML-M3_All.png){fig-align="center" width="550"}

\[[LINK](https://github.com/gu-ppol567-project/ppol-567-final-project-team-1/blob/main/code/ml/ml3.ipynb)\] - Model 3 Code

#### Saving model output and using for inference

The most accurate model of all modeling exercises was identified to be the random forest model from the third modeling exercise. This model was saved to our s3 bucket from the "ml_3.ipynb" notebook where the model was generated and reloaded into the notebook called "s3_predictions.ipynb" in our code/ml directory. This process demonstrated that we were successfully able to save a model and reload it to make predictions on new data without completing any additional training.

\[[LINK](https://github.com/gu-ppol567-project/ppol-567-final-project-team-1/blob/main/code/ml/s3_predictions.ipynb)\] to S3 Notebook

## Conclusion

The machine learning aspect of this project provided valuable insights into working with stock data. Stock discussions can be incredibly volatile and can shift focus quickly and without warning as stock market news spreads. Out of the three models constructed for this segment of the project, the logistic regression and random forest algorithms from the third modeling analysis yielded the highest accuracy score at 84%. This suggests that variables such as comment score, controversiality, length of comments, and sentiment were helpful predictors of a post's associated stock. However, we also discovered that the data was slightly unbalanced, with one class having more observations associated with it than the other. This could have influenced the high accuracy score, as the models ended up predicting only the negative class to achieve this score. Future adjustments could be made to rebalance the classes to achieve better modeling results.

However, the first two modeling exercises did not yield promising results. Our analysis of Reddit submissions that mentioned "basic girl" stocks revealed that information on a submission's score and number of comments, as well as information on which stocks were mentioned in the submission and their corresponding share price, were not very predictive of a submission's sentiment. Similarly, our analysis of comments taken from the subreddit `wallstreetbets` in February 2022 showed that information about the comments, including length, score, and controversiality, were only slightly more predictive of a comment's sentiment than chance.

In conclusion, analyzing these business questions highlighted the challenges in making accurate predictions about Reddit submissions that discuss "basic girl" stocks. The varying sentiments associated with stocks, coupled with the similarity in engagement metrics, make it difficult to create models that perform better than random guessing. However, the insights gained from these analyses can inform future research and modeling efforts in this field.
